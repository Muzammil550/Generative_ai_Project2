# -*- coding: utf-8 -*-
""""Depression Tweet Classification using Neural Network & Random Forest"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12sWPK4Uf_7053Ha2hMc3yRkfSkYA-cAn
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download NLTK data
nltk.download("stopwords")
nltk.download("wordnet")

# Initialize NLTK tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

# Load Sentence Transformer model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

import gdown
import pandas as pd

# Google Drive file ID (Extracted from the link)
file_id = "1lrw6c8ZPQEawXw7do1VbdwBY0wk01C8D"

# Generate direct download URL
url = f"https://drive.google.com/uc?id={file_id}"

# Download the file
output = "dataset.csv"
gdown.download(url, output, quiet=False)

# Read the CSV file
df = pd.read_csv(output)

# Display first few rows
print(df.head())

df.info()
df.describe()

# Drop unnecessary columns
df.drop(columns=["Unnamed: 0"], inplace=True)

df.dropna(inplace=True)

df.isna().sum()

# Text preprocessing function
def preprocess_text(text):
    words = text.split()
    cleaned_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]
    return " ".join(cleaned_words)

df["processed_text"] = df["text"].astype(str).apply(preprocess_text)

# Checking lemmanatization
df.head()

# Convert text into embeddings
print("Computing embeddings... ⏳")

# Convert text into embeddings
df["embeddings"] = df["processed_text"].apply(lambda x: embedding_model.encode(x))

# Convert embeddings list to NumPy array
X = np.vstack(df["embeddings"].values)  # Shape: (134349, 384)
y = df["sentiment"].values # Labels: (134349,)

# Train-test split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Create DataLoader
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)

# Neural Network Model
class DepressionClassifier(nn.Module):
    def __init__(self):
        super(DepressionClassifier, self).__init__()
        self.fc1 = nn.Linear(384, 128)  # Input size = 384 (MiniLM embedding)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 2)  # Output: 2 classes (Depressed, Not Depressed)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)  # No softmax (handled by CrossEntropyLoss)

# Initialize model, loss function, and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_nn = DepressionClassifier().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_nn.parameters(), lr=0.001)

# Training loop
epochs = 10
best_accuracy = 0

print("\n🔹 Training Neural Network... 🚀")
for epoch in range(epochs):
    model_nn.train()
    total_loss = 0
    for X_batch, y_batch in tqdm(train_loader):
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = model_nn(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Evaluate on test set
    model_nn.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model_nn(X_batch)
            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(y_batch.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    print(f"🔹 Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f} - Accuracy: {accuracy:.4f}")

# Save best model
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        torch.save(model_nn.state_dict(), "best_model.pth")
        print("✅ Best model saved!")

# Final Performance Metrics
print("\n🔹 Final Model Performance:")
print(classification_report(all_labels, all_preds))

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# Random Forest Performance
print("\n🔹 Random Forest Model Performance:")
print(classification_report(y_test, rf_preds))

"""**Best to use NN for the highest accuracy of 80%**"""

